{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LeNet architecture implementation\n",
    "\"\"\"\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5,5))\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_inits(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "    \n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prob(class_id=0, total_img=10):\n",
    "    \n",
    "    lines = []\n",
    "    for j in range(total_img):\n",
    "        row = []\n",
    "        for i in range(0,5):\n",
    "            if i== class_id:\n",
    "                row.append(random.randrange(85, 100))\n",
    "            else:\n",
    "                row.append(random.randrange(0, 10))\n",
    "        lines.append(row)\n",
    "\n",
    "    lines = torch.FloatTensor(lines)\n",
    "    sums = torch.sum(lines, dim=1)\n",
    "    lines_prob = torch.div(lines, sums.view(-1,1))\n",
    "#     print(lines_prob)\n",
    "    return lines_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, data_loader, scheduler, num_epochs=20):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                \n",
    "                for data in data_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        outputs = model(inputs)\n",
    "                        \n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "#                         print(preds, labels)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                scheduler.step()\n",
    "                \n",
    "                epoch_loss = running_loss / train_size\n",
    "                epoch_acc = running_corrects.double() / train_size\n",
    "\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format('Train', epoch_loss, epoch_acc))\n",
    "                    \n",
    "#             else:\n",
    "                \n",
    "#                 model.eval()   # Set model to evaluate mode\n",
    "                \n",
    "#                 running_loss = 0.0\n",
    "#                 running_corrects = 0\n",
    "                \n",
    "#                 for data in test_loader:\n",
    "#                     inputs, labels, idx = data\n",
    "#                     inputs = inputs.to(device)\n",
    "#                     labels = labels.to(device)\n",
    "#                     optimizer.zero_grad()\n",
    "#                     with torch.set_grad_enabled(False):\n",
    "#                         outputs = model(inputs)\n",
    "#                         _, preds = torch.max(outputs, 1)\n",
    "                        \n",
    "#                         loss = criterion(outputs, labels)\n",
    "                \n",
    "#                     running_loss += loss.item() * inputs.size(0)\n",
    "#                     running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "#                 epoch_loss = running_loss / test_size\n",
    "#                 epoch_acc = running_corrects.double() / test_size\n",
    "                \n",
    "#                 print('{} Loss: {:.4f} Acc: {:.4f}'.format('Val', epoch_loss, epoch_acc))\n",
    "#                 if epoch_acc > best_acc:\n",
    "#                     best_acc = epoch_acc\n",
    "#                     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "#         print()\n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "#     # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "#     return model, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_train(x, y, optimizer, model, epoch=1000):\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        \n",
    "        prob = torch.softmax(logits, -1)\n",
    "        loss = y * prob.log()\n",
    "        loss = - loss.sum(-1).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print('loss', loss.item())\n",
    "            \n",
    "    print(\"Image training finished...\")\n",
    "    x = torch.tanh(x)\n",
    "    return x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, optimizer, data_loader):\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "                \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / test_size\n",
    "    epoch_acc = running_corrects.double() / test_size\n",
    "\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format('Val', epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4)\n",
    "def create_dataset(t=5, img_per_class=20):\n",
    "    \n",
    "    images_per_class = img_per_class\n",
    "    images_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for k in range(t):\n",
    "        learning_rate = 0.1\n",
    "        model = LeNet()\n",
    "        model.apply(weight_inits)\n",
    "\n",
    "        # generating labels\n",
    "        cl1 = generate_prob(class_id=0, total_img=images_per_class)\n",
    "        cl2 = generate_prob(class_id=1, total_img=images_per_class)\n",
    "        cl3 = generate_prob(class_id=2, total_img=images_per_class)\n",
    "        cl4 = generate_prob(class_id=3, total_img=images_per_class)\n",
    "        cl5 = generate_prob(class_id=4, total_img=images_per_class)\n",
    "\n",
    "        y = torch.cat((cl1, cl2, cl3, cl4, cl5))\n",
    "        y = y[torch.randperm(y.size()[0])]\n",
    "        # generating randome images\n",
    "        x = torch.randn((y.size(0), 1, 28, 28), requires_grad=True)\n",
    "#         print(x.size(), y.size())\n",
    "        optimizer = optim.Adam([x], lr=learning_rate)\n",
    "\n",
    "        images, labels = image_train(x, y, optimizer, model, 1000)\n",
    "#         print(images.size(), labels.size())\n",
    "        images_list.append(images)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    image_tensor = images_list[0].data\n",
    "    label_tensor = labels_list[0]\n",
    "    for i in range(1, len(images_list)):\n",
    "        image_tensor = torch.cat((image_tensor, images_list[i].data))\n",
    "        label_tensor = torch.cat((label_tensor, labels_list[i]), axis=0)\n",
    "\n",
    "    print(\"Image size: \", image_tensor.size(), \"labels size: \", label_tensor.size())\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(image_tensor, torch.max(label_tensor,1)[1])\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.617632269859314\n",
      "loss 0.620517909526825\n",
      "loss 0.618781328201294\n",
      "loss 0.6182681322097778\n",
      "loss 0.6180297136306763\n",
      "loss 0.617892324924469\n",
      "loss 0.6178056001663208\n",
      "loss 0.6177469491958618\n",
      "loss 0.6177054047584534\n",
      "loss 0.6176744699478149\n",
      "Image training finished...\n",
      "loss 1.7837510108947754\n",
      "loss 0.6358120441436768\n",
      "loss 0.6337413787841797\n",
      "loss 0.6332343816757202\n",
      "loss 0.6330169439315796\n",
      "loss 0.6328999996185303\n",
      "loss 0.6328295469284058\n",
      "loss 0.632781445980072\n",
      "loss 0.6327474117279053\n",
      "loss 0.6327221989631653\n",
      "Image training finished...\n",
      "loss 1.6976447105407715\n",
      "loss 0.6294398903846741\n",
      "loss 0.6251103281974792\n",
      "loss 0.6241304874420166\n",
      "loss 0.6237480044364929\n",
      "loss 0.6235500574111938\n",
      "loss 0.6234266757965088\n",
      "loss 0.623345673084259\n",
      "loss 0.6232882738113403\n",
      "loss 0.6232452988624573\n",
      "Image training finished...\n",
      "loss 1.661729335784912\n",
      "loss 0.6142184138298035\n",
      "loss 0.6134577393531799\n",
      "loss 0.6131983995437622\n",
      "loss 0.6130697727203369\n",
      "loss 0.6129953265190125\n",
      "loss 0.6129472851753235\n",
      "loss 0.6129149198532104\n",
      "loss 0.612891674041748\n",
      "loss 0.6128743886947632\n",
      "Image training finished...\n",
      "loss 1.6287790536880493\n",
      "loss 0.6324678659439087\n",
      "loss 0.6316432952880859\n",
      "loss 0.631388247013092\n",
      "loss 0.6312649846076965\n",
      "loss 0.631195068359375\n",
      "loss 0.6311510801315308\n",
      "loss 0.6311209797859192\n",
      "loss 0.6310994625091553\n",
      "loss 0.631083607673645\n",
      "Image training finished...\n",
      "Image size:  torch.Size([750, 1, 28, 28]) labels size:  torch.Size([750, 5])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_dataset(t=5, img_per_class=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "Train Loss: 1.0754 Acc: 0.1200\n",
      "Epoch 1/49\n",
      "----------\n",
      "Train Loss: 1.0709 Acc: 0.1360\n",
      "Epoch 2/49\n",
      "----------\n",
      "Train Loss: 1.0673 Acc: 0.1920\n",
      "Epoch 3/49\n",
      "----------\n",
      "Train Loss: 1.0621 Acc: 0.1773\n",
      "Epoch 4/49\n",
      "----------\n",
      "Train Loss: 1.0553 Acc: 0.1627\n",
      "Epoch 5/49\n",
      "----------\n",
      "Train Loss: 1.0429 Acc: 0.2507\n",
      "Epoch 6/49\n",
      "----------\n",
      "Train Loss: 1.0170 Acc: 0.3213\n",
      "Epoch 7/49\n",
      "----------\n",
      "Train Loss: 0.9880 Acc: 0.3987\n",
      "Epoch 8/49\n",
      "----------\n",
      "Train Loss: 0.9813 Acc: 0.4360\n",
      "Epoch 9/49\n",
      "----------\n",
      "Train Loss: 0.9749 Acc: 0.4320\n",
      "Epoch 10/49\n",
      "----------\n",
      "Train Loss: 0.9687 Acc: 0.4213\n",
      "Epoch 11/49\n",
      "----------\n",
      "Train Loss: 0.9624 Acc: 0.4320\n",
      "Epoch 12/49\n",
      "----------\n",
      "Train Loss: 0.9560 Acc: 0.4373\n",
      "Epoch 13/49\n",
      "----------\n",
      "Train Loss: 0.9503 Acc: 0.4293\n",
      "Epoch 14/49\n",
      "----------\n",
      "Train Loss: 0.9450 Acc: 0.4307\n",
      "Epoch 15/49\n",
      "----------\n",
      "Train Loss: 0.9443 Acc: 0.4307\n",
      "Epoch 16/49\n",
      "----------\n",
      "Train Loss: 0.9436 Acc: 0.4307\n",
      "Epoch 17/49\n",
      "----------\n",
      "Train Loss: 0.9429 Acc: 0.4320\n",
      "Epoch 18/49\n",
      "----------\n",
      "Train Loss: 0.9424 Acc: 0.4320\n",
      "Epoch 19/49\n",
      "----------\n",
      "Train Loss: 0.9417 Acc: 0.4333\n",
      "Epoch 20/49\n",
      "----------\n",
      "Train Loss: 0.9411 Acc: 0.4347\n",
      "Epoch 21/49\n",
      "----------\n",
      "Train Loss: 0.9407 Acc: 0.4320\n",
      "Epoch 22/49\n",
      "----------\n",
      "Train Loss: 0.9407 Acc: 0.4320\n",
      "Epoch 23/49\n",
      "----------\n",
      "Train Loss: 0.9406 Acc: 0.4320\n",
      "Epoch 24/49\n",
      "----------\n",
      "Train Loss: 0.9406 Acc: 0.4320\n",
      "Epoch 25/49\n",
      "----------\n",
      "Train Loss: 0.9405 Acc: 0.4320\n",
      "Epoch 26/49\n",
      "----------\n",
      "Train Loss: 0.9404 Acc: 0.4320\n",
      "Epoch 27/49\n",
      "----------\n",
      "Train Loss: 0.9404 Acc: 0.4320\n",
      "Epoch 28/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 29/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 30/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 31/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 32/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 33/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 34/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 35/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 36/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 37/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 38/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 39/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 40/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 41/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 42/49\n",
      "----------\n",
      "Train Loss: 0.9403 Acc: 0.4320\n",
      "Epoch 43/49\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "init_dataset = torch.utils.data.TensorDataset(image_tensor, torch.max(label_tensor,1)[1])\n",
    "\n",
    "lengths = [int(len(init_dataset)*1.0), int(len(init_dataset)*0.0)]\n",
    "train_set, test_set = torch.utils.data.dataset.random_split(init_dataset, lengths)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=0)\n",
    "# test_loader = DataLoader(test_set, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "test_size = len(test_dataset)\n",
    "# net = torch.load('LeNet_5_class_pretrained_model.tar')\n",
    "net = LeNet()\n",
    "# net.apply(weight_inits)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "train_model(net, criterion, optimizer_ft, train_loader, exp_lr_scheduler,\n",
    "                       num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2604 Acc: 0.0400\n"
     ]
    }
   ],
   "source": [
    "test_model(net, criterion, optimizer_ft, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Installation\\anaconda\\envs\\dl_env\\lib\\site-packages\\torch\\serialization.py:401: UserWarning: Couldn't retrieve source code for container of type LeNet. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, 'pretrained_net_on_generated_dataset.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.6802637577056885\n",
      "loss 0.7105974555015564\n",
      "loss 0.6532875299453735\n",
      "loss 0.6380001902580261\n",
      "loss 0.6321803331375122\n",
      "loss 0.6293888688087463\n",
      "loss 0.6278557777404785\n",
      "loss 0.6269125938415527\n",
      "loss 0.6262786984443665\n",
      "loss 0.6258309483528137\n",
      "Image training finished...\n",
      "Image size:  torch.Size([150, 1, 28, 28]) labels size:  torch.Size([150, 5])\n"
     ]
    }
   ],
   "source": [
    "test_dataset = create_dataset(t=1, img_per_class=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6512 Acc: 0.1533\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "test_model(net, criterion, optimizer_ft, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24cf4f515e0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = images.detach().numpy()\n",
    "z = y[0,0,:,:]\n",
    "plt.imshow(z, cmap='Greys', interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
